{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L04yqyMjCYMX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ML Project/TeamStats.csv')"
      ],
      "metadata": {
        "id": "r5F_BSajCdmO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "KkUVYpcyChvI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "qf0NvA0sCkGi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = 0.8\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_std, y, train_size=train_split, random_state=42)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "id": "nEvPaI2wCmDW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinReg(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinReg, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "SKl4DVTQCvnv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(train_loader, test_loader, input_size, output_size, learning_rate, epochs):\n",
        "    model = LinReg(input_size, output_size)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            y_batch = y_batch.view_as(y_pred)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_values = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            predictions.extend(y_pred.numpy())\n",
        "            true_values.extend(y_batch.numpy())\n",
        "\n",
        "    predictions = np.array(predictions).reshape(-1)\n",
        "    true_values = np.array(true_values).reshape(-1)\n",
        "    mse = np.mean((predictions - true_values) ** 2)\n",
        "    return mse"
      ],
      "metadata": {
        "id": "2zkP9ORLCyTZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.005, 0.01, 0.05],\n",
        "    'batch_size': [1, 2, 5, 10, 30],\n",
        "    'epochs': [50, 100, 150, 200, 250]\n",
        "}\n",
        "\n",
        "best_mse = float('inf')\n",
        "best_params = None\n",
        "\n",
        "input_size = X_train_tensor.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    print(f'Testing parameters: {params}')\n",
        "    mse = train_and_evaluate(train_loader, test_loader, input_size, output_size, params['learning_rate'], params['epochs'])\n",
        "    print(f'Means Squared Error: {mse}')\n",
        "\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_params = params\n",
        "\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best Mean Squared Error: {best_mse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt_2uWmfC1w9",
        "outputId": "4969c4ad-5f6b-4983-9f2c-bc72ffd4407c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing parameters: {'batch_size': 1, 'epochs': 50, 'learning_rate': 0.001}\n",
            "Epoch [10/50], Loss: 1112.6698\n",
            "Epoch [20/50], Loss: 36.9964\n",
            "Epoch [30/50], Loss: 16.4918\n",
            "Epoch [40/50], Loss: 12.6258\n",
            "Epoch [50/50], Loss: 0.3503\n",
            "Means Squared Error: 10.642772674560547\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 50, 'learning_rate': 0.005}\n",
            "Epoch [10/50], Loss: 3.6461\n",
            "Epoch [20/50], Loss: 0.3551\n",
            "Epoch [30/50], Loss: 0.8911\n",
            "Epoch [40/50], Loss: 0.0173\n",
            "Epoch [50/50], Loss: 0.0499\n",
            "Means Squared Error: 0.41347527503967285\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 50, 'learning_rate': 0.01}\n",
            "Epoch [10/50], Loss: 0.2719\n",
            "Epoch [20/50], Loss: 0.0215\n",
            "Epoch [30/50], Loss: 0.0102\n",
            "Epoch [40/50], Loss: 0.0045\n",
            "Epoch [50/50], Loss: 0.3122\n",
            "Means Squared Error: 0.4451717436313629\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 50, 'learning_rate': 0.05}\n",
            "Epoch [10/50], Loss: 143495.0781\n",
            "Epoch [20/50], Loss: 2538065887232.0000\n",
            "Epoch [30/50], Loss: 5210138652553773056.0000\n",
            "Epoch [40/50], Loss: 47369455355833689833472.0000\n",
            "Epoch [50/50], Loss: 115335527979374025610625024.0000\n",
            "Means Squared Error: 5.270982612863567e+26\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 100, 'learning_rate': 0.001}\n",
            "Epoch [10/100], Loss: 1510.1270\n",
            "Epoch [20/100], Loss: 311.0775\n",
            "Epoch [30/100], Loss: 2.7196\n",
            "Epoch [40/100], Loss: 9.3725\n",
            "Epoch [50/100], Loss: 0.0420\n",
            "Epoch [60/100], Loss: 2.1261\n",
            "Epoch [70/100], Loss: 0.0016\n",
            "Epoch [80/100], Loss: 2.2913\n",
            "Epoch [90/100], Loss: 0.1287\n",
            "Epoch [100/100], Loss: 0.4697\n",
            "Means Squared Error: 1.5276631116867065\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 100, 'learning_rate': 0.005}\n",
            "Epoch [10/100], Loss: 0.0604\n",
            "Epoch [20/100], Loss: 3.7610\n",
            "Epoch [30/100], Loss: 0.0060\n",
            "Epoch [40/100], Loss: 0.0903\n",
            "Epoch [50/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0060\n",
            "Epoch [70/100], Loss: 0.0087\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0035\n",
            "Epoch [100/100], Loss: 0.0975\n",
            "Means Squared Error: 0.29260462522506714\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 100, 'learning_rate': 0.01}\n",
            "Epoch [10/100], Loss: 0.2680\n",
            "Epoch [20/100], Loss: 0.5461\n",
            "Epoch [30/100], Loss: 0.2265\n",
            "Epoch [40/100], Loss: 0.1044\n",
            "Epoch [50/100], Loss: 0.3939\n",
            "Epoch [60/100], Loss: 0.0015\n",
            "Epoch [70/100], Loss: 0.3319\n",
            "Epoch [80/100], Loss: 1.1490\n",
            "Epoch [90/100], Loss: 0.0502\n",
            "Epoch [100/100], Loss: 0.0380\n",
            "Means Squared Error: 0.20819826424121857\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 100, 'learning_rate': 0.05}\n",
            "Epoch [10/100], Loss: 212482528.0000\n",
            "Epoch [20/100], Loss: 1317303353344.0000\n",
            "Epoch [30/100], Loss: 402487570061066240.0000\n",
            "Epoch [40/100], Loss: 8952187605148676296540160.0000\n",
            "Epoch [50/100], Loss: 68467331908977956533305344.0000\n",
            "Epoch [60/100], Loss: 111885949205642133280589021708288.0000\n",
            "Epoch [70/100], Loss: 3205137284996556930464775609746718720.0000\n",
            "Epoch [80/100], Loss: inf\n",
            "Epoch [90/100], Loss: inf\n",
            "Epoch [100/100], Loss: inf\n",
            "Means Squared Error: inf\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 150, 'learning_rate': 0.001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-d3e7f04d9168>:32: RuntimeWarning: overflow encountered in square\n",
            "  mse = np.mean((predictions - true_values) ** 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/150], Loss: 819.8314\n",
            "Epoch [20/150], Loss: 42.7477\n",
            "Epoch [30/150], Loss: 0.0553\n",
            "Epoch [40/150], Loss: 0.3756\n",
            "Epoch [50/150], Loss: 6.3128\n",
            "Epoch [60/150], Loss: 2.3381\n",
            "Epoch [70/150], Loss: 0.0554\n",
            "Epoch [80/150], Loss: 3.7798\n",
            "Epoch [90/150], Loss: 0.7357\n",
            "Epoch [100/150], Loss: 0.8879\n",
            "Epoch [110/150], Loss: 0.0047\n",
            "Epoch [120/150], Loss: 0.0034\n",
            "Epoch [130/150], Loss: 0.9031\n",
            "Epoch [140/150], Loss: 0.5373\n",
            "Epoch [150/150], Loss: 0.0054\n",
            "Means Squared Error: 0.6035837531089783\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 150, 'learning_rate': 0.005}\n",
            "Epoch [10/150], Loss: 0.8788\n",
            "Epoch [20/150], Loss: 0.0032\n",
            "Epoch [30/150], Loss: 0.0039\n",
            "Epoch [40/150], Loss: 0.0323\n",
            "Epoch [50/150], Loss: 0.0712\n",
            "Epoch [60/150], Loss: 0.0280\n",
            "Epoch [70/150], Loss: 0.3537\n",
            "Epoch [80/150], Loss: 0.2358\n",
            "Epoch [90/150], Loss: 0.0104\n",
            "Epoch [100/150], Loss: 0.1042\n",
            "Epoch [110/150], Loss: 0.2188\n",
            "Epoch [120/150], Loss: 0.0014\n",
            "Epoch [130/150], Loss: 0.0265\n",
            "Epoch [140/150], Loss: 0.0340\n",
            "Epoch [150/150], Loss: 0.0247\n",
            "Means Squared Error: 0.2630729079246521\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 150, 'learning_rate': 0.01}\n",
            "Epoch [10/150], Loss: 0.1315\n",
            "Epoch [20/150], Loss: 0.1815\n",
            "Epoch [30/150], Loss: 0.0072\n",
            "Epoch [40/150], Loss: 0.0733\n",
            "Epoch [50/150], Loss: 0.0084\n",
            "Epoch [60/150], Loss: 0.8457\n",
            "Epoch [70/150], Loss: 0.0076\n",
            "Epoch [80/150], Loss: 0.2574\n",
            "Epoch [90/150], Loss: 0.0247\n",
            "Epoch [100/150], Loss: 0.2804\n",
            "Epoch [110/150], Loss: 0.1804\n",
            "Epoch [120/150], Loss: 0.0072\n",
            "Epoch [130/150], Loss: 0.0645\n",
            "Epoch [140/150], Loss: 0.1429\n",
            "Epoch [150/150], Loss: 0.0159\n",
            "Means Squared Error: 0.12559859454631805\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 150, 'learning_rate': 0.05}\n",
            "Epoch [10/150], Loss: 321877792.0000\n",
            "Epoch [20/150], Loss: 83347204210688.0000\n",
            "Epoch [30/150], Loss: 112983694901248.0000\n",
            "Epoch [40/150], Loss: 479402234174706286592.0000\n",
            "Epoch [50/150], Loss: 97752478909939628910837760.0000\n",
            "Epoch [60/150], Loss: 249513052219618968633316737024.0000\n",
            "Epoch [70/150], Loss: 2277539477364415836193841741824.0000\n",
            "Epoch [80/150], Loss: 89922032554175275347064502655675006976.0000\n",
            "Epoch [90/150], Loss: inf\n",
            "Epoch [100/150], Loss: inf\n",
            "Epoch [110/150], Loss: inf\n",
            "Epoch [120/150], Loss: inf\n",
            "Epoch [130/150], Loss: inf\n",
            "Epoch [140/150], Loss: inf\n",
            "Epoch [150/150], Loss: inf\n",
            "Means Squared Error: inf\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 200, 'learning_rate': 0.001}\n",
            "Epoch [10/200], Loss: 604.1274\n",
            "Epoch [20/200], Loss: 65.7433\n",
            "Epoch [30/200], Loss: 8.2330\n",
            "Epoch [40/200], Loss: 14.9920\n",
            "Epoch [50/200], Loss: 4.1009\n",
            "Epoch [60/200], Loss: 0.3026\n",
            "Epoch [70/200], Loss: 6.0474\n",
            "Epoch [80/200], Loss: 0.3529\n",
            "Epoch [90/200], Loss: 0.4233\n",
            "Epoch [100/200], Loss: 2.8620\n",
            "Epoch [110/200], Loss: 0.0093\n",
            "Epoch [120/200], Loss: 0.4691\n",
            "Epoch [130/200], Loss: 0.0095\n",
            "Epoch [140/200], Loss: 0.3610\n",
            "Epoch [150/200], Loss: 0.5640\n",
            "Epoch [160/200], Loss: 0.0343\n",
            "Epoch [170/200], Loss: 0.0004\n",
            "Epoch [180/200], Loss: 0.0600\n",
            "Epoch [190/200], Loss: 0.1288\n",
            "Epoch [200/200], Loss: 0.0003\n",
            "Means Squared Error: 0.43272948265075684\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 200, 'learning_rate': 0.005}\n",
            "Epoch [10/200], Loss: 0.0140\n",
            "Epoch [20/200], Loss: 0.1980\n",
            "Epoch [30/200], Loss: 0.0691\n",
            "Epoch [40/200], Loss: 0.0208\n",
            "Epoch [50/200], Loss: 0.0403\n",
            "Epoch [60/200], Loss: 0.0005\n",
            "Epoch [70/200], Loss: 0.2145\n",
            "Epoch [80/200], Loss: 0.0313\n",
            "Epoch [90/200], Loss: 0.4518\n",
            "Epoch [100/200], Loss: 0.1754\n",
            "Epoch [110/200], Loss: 0.0043\n",
            "Epoch [120/200], Loss: 0.0398\n",
            "Epoch [130/200], Loss: 0.0088\n",
            "Epoch [140/200], Loss: 0.0016\n",
            "Epoch [150/200], Loss: 0.2552\n",
            "Epoch [160/200], Loss: 0.0001\n",
            "Epoch [170/200], Loss: 1.0967\n",
            "Epoch [180/200], Loss: 0.6083\n",
            "Epoch [190/200], Loss: 0.0002\n",
            "Epoch [200/200], Loss: 0.0036\n",
            "Means Squared Error: 0.2421298325061798\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 200, 'learning_rate': 0.01}\n",
            "Epoch [10/200], Loss: 0.1207\n",
            "Epoch [20/200], Loss: 0.1002\n",
            "Epoch [30/200], Loss: 0.2465\n",
            "Epoch [40/200], Loss: 0.0658\n",
            "Epoch [50/200], Loss: 0.0036\n",
            "Epoch [60/200], Loss: 0.0774\n",
            "Epoch [70/200], Loss: 0.0067\n",
            "Epoch [80/200], Loss: 0.0001\n",
            "Epoch [90/200], Loss: 0.6504\n",
            "Epoch [100/200], Loss: 0.1147\n",
            "Epoch [110/200], Loss: 0.0019\n",
            "Epoch [120/200], Loss: 0.0067\n",
            "Epoch [130/200], Loss: 0.1544\n",
            "Epoch [140/200], Loss: 0.0270\n",
            "Epoch [150/200], Loss: 0.1087\n",
            "Epoch [160/200], Loss: 0.0023\n",
            "Epoch [170/200], Loss: 0.0069\n",
            "Epoch [180/200], Loss: 0.0939\n",
            "Epoch [190/200], Loss: 0.4184\n",
            "Epoch [200/200], Loss: 0.0067\n",
            "Means Squared Error: 0.18375591933727264\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 200, 'learning_rate': 0.05}\n",
            "Epoch [10/200], Loss: 10208509.0000\n",
            "Epoch [20/200], Loss: 10601308160.0000\n",
            "Epoch [30/200], Loss: 18960387527694876672.0000\n",
            "Epoch [40/200], Loss: 6632102556588573963845632.0000\n",
            "Epoch [50/200], Loss: 2105322916635751204670930944.0000\n",
            "Epoch [60/200], Loss: 21645652386288467783546321240064.0000\n",
            "Epoch [70/200], Loss: 467753920631446251621291428457480192.0000\n",
            "Epoch [80/200], Loss: inf\n",
            "Epoch [90/200], Loss: inf\n",
            "Epoch [100/200], Loss: inf\n",
            "Epoch [110/200], Loss: inf\n",
            "Epoch [120/200], Loss: inf\n",
            "Epoch [130/200], Loss: inf\n",
            "Epoch [140/200], Loss: inf\n",
            "Epoch [150/200], Loss: inf\n",
            "Epoch [160/200], Loss: nan\n",
            "Epoch [170/200], Loss: nan\n",
            "Epoch [180/200], Loss: nan\n",
            "Epoch [190/200], Loss: nan\n",
            "Epoch [200/200], Loss: nan\n",
            "Means Squared Error: nan\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 250, 'learning_rate': 0.001}\n",
            "Epoch [10/250], Loss: 1500.9719\n",
            "Epoch [20/250], Loss: 51.9945\n",
            "Epoch [30/250], Loss: 41.4651\n",
            "Epoch [40/250], Loss: 2.7664\n",
            "Epoch [50/250], Loss: 0.1273\n",
            "Epoch [60/250], Loss: 1.4822\n",
            "Epoch [70/250], Loss: 0.7088\n",
            "Epoch [80/250], Loss: 0.9079\n",
            "Epoch [90/250], Loss: 2.1752\n",
            "Epoch [100/250], Loss: 0.0031\n",
            "Epoch [110/250], Loss: 0.0093\n",
            "Epoch [120/250], Loss: 0.0589\n",
            "Epoch [130/250], Loss: 0.0376\n",
            "Epoch [140/250], Loss: 0.0070\n",
            "Epoch [150/250], Loss: 0.1515\n",
            "Epoch [160/250], Loss: 0.2088\n",
            "Epoch [170/250], Loss: 0.0656\n",
            "Epoch [180/250], Loss: 0.0011\n",
            "Epoch [190/250], Loss: 0.1220\n",
            "Epoch [200/250], Loss: 0.2803\n",
            "Epoch [210/250], Loss: 0.0692\n",
            "Epoch [220/250], Loss: 0.0608\n",
            "Epoch [230/250], Loss: 0.4557\n",
            "Epoch [240/250], Loss: 0.0055\n",
            "Epoch [250/250], Loss: 0.0011\n",
            "Means Squared Error: 0.36500731110572815\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 250, 'learning_rate': 0.005}\n",
            "Epoch [10/250], Loss: 4.8430\n",
            "Epoch [20/250], Loss: 0.7867\n",
            "Epoch [30/250], Loss: 1.0240\n",
            "Epoch [40/250], Loss: 0.1606\n",
            "Epoch [50/250], Loss: 0.0448\n",
            "Epoch [60/250], Loss: 0.0298\n",
            "Epoch [70/250], Loss: 0.1876\n",
            "Epoch [80/250], Loss: 0.3270\n",
            "Epoch [90/250], Loss: 0.0023\n",
            "Epoch [100/250], Loss: 0.0919\n",
            "Epoch [110/250], Loss: 0.0014\n",
            "Epoch [120/250], Loss: 0.0715\n",
            "Epoch [130/250], Loss: 0.0130\n",
            "Epoch [140/250], Loss: 0.0068\n",
            "Epoch [150/250], Loss: 1.0471\n",
            "Epoch [160/250], Loss: 0.2243\n",
            "Epoch [170/250], Loss: 0.0007\n",
            "Epoch [180/250], Loss: 0.0019\n",
            "Epoch [190/250], Loss: 0.0032\n",
            "Epoch [200/250], Loss: 0.0210\n",
            "Epoch [210/250], Loss: 0.0109\n",
            "Epoch [220/250], Loss: 0.1015\n",
            "Epoch [230/250], Loss: 0.1779\n",
            "Epoch [240/250], Loss: 0.0309\n",
            "Epoch [250/250], Loss: 0.0034\n",
            "Means Squared Error: 0.20496556162834167\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 250, 'learning_rate': 0.01}\n",
            "Epoch [10/250], Loss: 0.0000\n",
            "Epoch [20/250], Loss: 0.0931\n",
            "Epoch [30/250], Loss: 0.6416\n",
            "Epoch [40/250], Loss: 0.2324\n",
            "Epoch [50/250], Loss: 0.0639\n",
            "Epoch [60/250], Loss: 0.0017\n",
            "Epoch [70/250], Loss: 0.2041\n",
            "Epoch [80/250], Loss: 0.0678\n",
            "Epoch [90/250], Loss: 0.0002\n",
            "Epoch [100/250], Loss: 0.1293\n",
            "Epoch [110/250], Loss: 0.0001\n",
            "Epoch [120/250], Loss: 0.1626\n",
            "Epoch [130/250], Loss: 0.0114\n",
            "Epoch [140/250], Loss: 0.0705\n",
            "Epoch [150/250], Loss: 0.0695\n",
            "Epoch [160/250], Loss: 0.0222\n",
            "Epoch [170/250], Loss: 0.0274\n",
            "Epoch [180/250], Loss: 0.0015\n",
            "Epoch [190/250], Loss: 0.0145\n",
            "Epoch [200/250], Loss: 0.0074\n",
            "Epoch [210/250], Loss: 0.0453\n",
            "Epoch [220/250], Loss: 0.0156\n",
            "Epoch [230/250], Loss: 0.0018\n",
            "Epoch [240/250], Loss: 0.0494\n",
            "Epoch [250/250], Loss: 0.0064\n",
            "Means Squared Error: 0.10932734608650208\n",
            "Testing parameters: {'batch_size': 1, 'epochs': 250, 'learning_rate': 0.05}\n",
            "Epoch [10/250], Loss: 266112466944.0000\n",
            "Epoch [20/250], Loss: 606723991666688.0000\n",
            "Epoch [30/250], Loss: 105091553311064064.0000\n",
            "Epoch [40/250], Loss: 583177378566437339136.0000\n",
            "Epoch [50/250], Loss: 67565945594874194575753216.0000\n",
            "Epoch [60/250], Loss: 18783293230296745662144604798976.0000\n",
            "Epoch [70/250], Loss: 3264320088569412276532452192296108032.0000\n",
            "Epoch [80/250], Loss: inf\n",
            "Epoch [90/250], Loss: inf\n",
            "Epoch [100/250], Loss: inf\n",
            "Epoch [110/250], Loss: inf\n",
            "Epoch [120/250], Loss: inf\n",
            "Epoch [130/250], Loss: inf\n",
            "Epoch [140/250], Loss: inf\n",
            "Epoch [150/250], Loss: nan\n",
            "Epoch [160/250], Loss: nan\n",
            "Epoch [170/250], Loss: nan\n",
            "Epoch [180/250], Loss: nan\n",
            "Epoch [190/250], Loss: nan\n",
            "Epoch [200/250], Loss: nan\n",
            "Epoch [210/250], Loss: nan\n",
            "Epoch [220/250], Loss: nan\n",
            "Epoch [230/250], Loss: nan\n",
            "Epoch [240/250], Loss: nan\n",
            "Epoch [250/250], Loss: nan\n",
            "Means Squared Error: nan\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 50, 'learning_rate': 0.001}\n",
            "Epoch [10/50], Loss: 2743.1230\n",
            "Epoch [20/50], Loss: 1271.0070\n",
            "Epoch [30/50], Loss: 141.6039\n",
            "Epoch [40/50], Loss: 109.5559\n",
            "Epoch [50/50], Loss: 29.7473\n",
            "Means Squared Error: 99.8614501953125\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 50, 'learning_rate': 0.005}\n",
            "Epoch [10/50], Loss: 160.2376\n",
            "Epoch [20/50], Loss: 0.0005\n",
            "Epoch [30/50], Loss: 0.0216\n",
            "Epoch [40/50], Loss: 0.4057\n",
            "Epoch [50/50], Loss: 0.0267\n",
            "Means Squared Error: 0.7708901762962341\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 50, 'learning_rate': 0.01}\n",
            "Epoch [10/50], Loss: 3.5246\n",
            "Epoch [20/50], Loss: 0.4050\n",
            "Epoch [30/50], Loss: 0.0232\n",
            "Epoch [40/50], Loss: 3.4746\n",
            "Epoch [50/50], Loss: 0.0142\n",
            "Means Squared Error: 0.47018206119537354\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 50, 'learning_rate': 0.05}\n",
            "Epoch [10/50], Loss: 0.0023\n",
            "Epoch [20/50], Loss: 0.0319\n",
            "Epoch [30/50], Loss: 0.0030\n",
            "Epoch [40/50], Loss: 0.0678\n",
            "Epoch [50/50], Loss: 0.0746\n",
            "Means Squared Error: 0.34215879440307617\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 100, 'learning_rate': 0.001}\n",
            "Epoch [10/100], Loss: 4161.5986\n",
            "Epoch [20/100], Loss: 712.2300\n",
            "Epoch [30/100], Loss: 69.6936\n",
            "Epoch [40/100], Loss: 280.6515\n",
            "Epoch [50/100], Loss: 0.8482\n",
            "Epoch [60/100], Loss: 12.4695\n",
            "Epoch [70/100], Loss: 5.6308\n",
            "Epoch [80/100], Loss: 0.0341\n",
            "Epoch [90/100], Loss: 0.5775\n",
            "Epoch [100/100], Loss: 12.4305\n",
            "Means Squared Error: 10.171012878417969\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 100, 'learning_rate': 0.005}\n",
            "Epoch [10/100], Loss: 14.9883\n",
            "Epoch [20/100], Loss: 6.3999\n",
            "Epoch [30/100], Loss: 1.0101\n",
            "Epoch [40/100], Loss: 0.5306\n",
            "Epoch [50/100], Loss: 0.0161\n",
            "Epoch [60/100], Loss: 0.6262\n",
            "Epoch [70/100], Loss: 0.0605\n",
            "Epoch [80/100], Loss: 0.7292\n",
            "Epoch [90/100], Loss: 0.0184\n",
            "Epoch [100/100], Loss: 0.2313\n",
            "Means Squared Error: 0.3678866922855377\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 100, 'learning_rate': 0.01}\n",
            "Epoch [10/100], Loss: 5.3441\n",
            "Epoch [20/100], Loss: 0.2168\n",
            "Epoch [30/100], Loss: 0.0928\n",
            "Epoch [40/100], Loss: 0.0522\n",
            "Epoch [50/100], Loss: 0.5677\n",
            "Epoch [60/100], Loss: 0.2917\n",
            "Epoch [70/100], Loss: 0.0037\n",
            "Epoch [80/100], Loss: 0.2859\n",
            "Epoch [90/100], Loss: 0.8844\n",
            "Epoch [100/100], Loss: 0.2039\n",
            "Means Squared Error: 0.32065093517303467\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 100, 'learning_rate': 0.05}\n",
            "Epoch [10/100], Loss: 0.9124\n",
            "Epoch [20/100], Loss: 0.0243\n",
            "Epoch [30/100], Loss: 0.3514\n",
            "Epoch [40/100], Loss: 0.3263\n",
            "Epoch [50/100], Loss: 0.1058\n",
            "Epoch [60/100], Loss: 0.0442\n",
            "Epoch [70/100], Loss: 0.0716\n",
            "Epoch [80/100], Loss: 0.0242\n",
            "Epoch [90/100], Loss: 0.0120\n",
            "Epoch [100/100], Loss: 0.0519\n",
            "Means Squared Error: 0.1931493729352951\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 150, 'learning_rate': 0.001}\n",
            "Epoch [10/150], Loss: 2947.4158\n",
            "Epoch [20/150], Loss: 814.0640\n",
            "Epoch [30/150], Loss: 406.9021\n",
            "Epoch [40/150], Loss: 5.4949\n",
            "Epoch [50/150], Loss: 23.7949\n",
            "Epoch [60/150], Loss: 7.4109\n",
            "Epoch [70/150], Loss: 24.1705\n",
            "Epoch [80/150], Loss: 0.3321\n",
            "Epoch [90/150], Loss: 2.4372\n",
            "Epoch [100/150], Loss: 12.9053\n",
            "Epoch [110/150], Loss: 6.1177\n",
            "Epoch [120/150], Loss: 0.2256\n",
            "Epoch [130/150], Loss: 0.5703\n",
            "Epoch [140/150], Loss: 0.5494\n",
            "Epoch [150/150], Loss: 1.0956\n",
            "Means Squared Error: 3.4743564128875732\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 150, 'learning_rate': 0.005}\n",
            "Epoch [10/150], Loss: 57.4519\n",
            "Epoch [20/150], Loss: 7.0261\n",
            "Epoch [30/150], Loss: 1.5247\n",
            "Epoch [40/150], Loss: 2.0463\n",
            "Epoch [50/150], Loss: 0.0889\n",
            "Epoch [60/150], Loss: 0.0032\n",
            "Epoch [70/150], Loss: 0.0300\n",
            "Epoch [80/150], Loss: 0.0183\n",
            "Epoch [90/150], Loss: 0.0477\n",
            "Epoch [100/150], Loss: 0.0805\n",
            "Epoch [110/150], Loss: 0.0054\n",
            "Epoch [120/150], Loss: 0.0621\n",
            "Epoch [130/150], Loss: 0.2435\n",
            "Epoch [140/150], Loss: 0.0228\n",
            "Epoch [150/150], Loss: 0.0029\n",
            "Means Squared Error: 0.33237552642822266\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 150, 'learning_rate': 0.01}\n",
            "Epoch [10/150], Loss: 1.5393\n",
            "Epoch [20/150], Loss: 2.0368\n",
            "Epoch [30/150], Loss: 0.0063\n",
            "Epoch [40/150], Loss: 0.0662\n",
            "Epoch [50/150], Loss: 0.1519\n",
            "Epoch [60/150], Loss: 0.2312\n",
            "Epoch [70/150], Loss: 0.2529\n",
            "Epoch [80/150], Loss: 0.0004\n",
            "Epoch [90/150], Loss: 0.6155\n",
            "Epoch [100/150], Loss: 0.0428\n",
            "Epoch [110/150], Loss: 0.0016\n",
            "Epoch [120/150], Loss: 0.0717\n",
            "Epoch [130/150], Loss: 0.0209\n",
            "Epoch [140/150], Loss: 0.0280\n",
            "Epoch [150/150], Loss: 0.0173\n",
            "Means Squared Error: 0.2877274453639984\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 150, 'learning_rate': 0.05}\n",
            "Epoch [10/150], Loss: 0.0929\n",
            "Epoch [20/150], Loss: 0.0018\n",
            "Epoch [30/150], Loss: 0.0141\n",
            "Epoch [40/150], Loss: 0.3338\n",
            "Epoch [50/150], Loss: 0.0000\n",
            "Epoch [60/150], Loss: 0.1709\n",
            "Epoch [70/150], Loss: 0.0100\n",
            "Epoch [80/150], Loss: 0.0100\n",
            "Epoch [90/150], Loss: 0.0315\n",
            "Epoch [100/150], Loss: 0.1170\n",
            "Epoch [110/150], Loss: 0.0078\n",
            "Epoch [120/150], Loss: 0.0064\n",
            "Epoch [130/150], Loss: 0.0017\n",
            "Epoch [140/150], Loss: 0.0045\n",
            "Epoch [150/150], Loss: 0.0364\n",
            "Means Squared Error: 0.06799612194299698\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 200, 'learning_rate': 0.001}\n",
            "Epoch [10/200], Loss: 3996.1877\n",
            "Epoch [20/200], Loss: 1418.0753\n",
            "Epoch [30/200], Loss: 538.3040\n",
            "Epoch [40/200], Loss: 305.7874\n",
            "Epoch [50/200], Loss: 20.5969\n",
            "Epoch [60/200], Loss: 0.0725\n",
            "Epoch [70/200], Loss: 2.3448\n",
            "Epoch [80/200], Loss: 1.7910\n",
            "Epoch [90/200], Loss: 8.0554\n",
            "Epoch [100/200], Loss: 2.4243\n",
            "Epoch [110/200], Loss: 3.9759\n",
            "Epoch [120/200], Loss: 0.2248\n",
            "Epoch [130/200], Loss: 12.3075\n",
            "Epoch [140/200], Loss: 1.0252\n",
            "Epoch [150/200], Loss: 2.4970\n",
            "Epoch [160/200], Loss: 0.0855\n",
            "Epoch [170/200], Loss: 0.8658\n",
            "Epoch [180/200], Loss: 0.2355\n",
            "Epoch [190/200], Loss: 1.3538\n",
            "Epoch [200/200], Loss: 0.7878\n",
            "Means Squared Error: 1.4807145595550537\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 200, 'learning_rate': 0.005}\n",
            "Epoch [10/200], Loss: 16.4785\n",
            "Epoch [20/200], Loss: 15.4328\n",
            "Epoch [30/200], Loss: 0.3450\n",
            "Epoch [40/200], Loss: 1.1328\n",
            "Epoch [50/200], Loss: 0.0061\n",
            "Epoch [60/200], Loss: 0.7097\n",
            "Epoch [70/200], Loss: 0.0027\n",
            "Epoch [80/200], Loss: 0.0634\n",
            "Epoch [90/200], Loss: 0.1215\n",
            "Epoch [100/200], Loss: 0.0109\n",
            "Epoch [110/200], Loss: 0.0164\n",
            "Epoch [120/200], Loss: 0.0487\n",
            "Epoch [130/200], Loss: 0.0271\n",
            "Epoch [140/200], Loss: 0.0543\n",
            "Epoch [150/200], Loss: 0.0001\n",
            "Epoch [160/200], Loss: 0.4028\n",
            "Epoch [170/200], Loss: 0.0071\n",
            "Epoch [180/200], Loss: 0.0101\n",
            "Epoch [190/200], Loss: 0.0098\n",
            "Epoch [200/200], Loss: 0.0006\n",
            "Means Squared Error: 0.34188634157180786\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 200, 'learning_rate': 0.01}\n",
            "Epoch [10/200], Loss: 3.5119\n",
            "Epoch [20/200], Loss: 1.4496\n",
            "Epoch [30/200], Loss: 0.0736\n",
            "Epoch [40/200], Loss: 0.1383\n",
            "Epoch [50/200], Loss: 0.4934\n",
            "Epoch [60/200], Loss: 0.9956\n",
            "Epoch [70/200], Loss: 0.1136\n",
            "Epoch [80/200], Loss: 0.1728\n",
            "Epoch [90/200], Loss: 0.0996\n",
            "Epoch [100/200], Loss: 0.0031\n",
            "Epoch [110/200], Loss: 0.0109\n",
            "Epoch [120/200], Loss: 0.0262\n",
            "Epoch [130/200], Loss: 0.0018\n",
            "Epoch [140/200], Loss: 0.0840\n",
            "Epoch [150/200], Loss: 0.1698\n",
            "Epoch [160/200], Loss: 0.1700\n",
            "Epoch [170/200], Loss: 0.0087\n",
            "Epoch [180/200], Loss: 0.0016\n",
            "Epoch [190/200], Loss: 0.4058\n",
            "Epoch [200/200], Loss: 0.0066\n",
            "Means Squared Error: 0.20184506475925446\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 200, 'learning_rate': 0.05}\n",
            "Epoch [10/200], Loss: 0.1618\n",
            "Epoch [20/200], Loss: 0.0240\n",
            "Epoch [30/200], Loss: 0.2627\n",
            "Epoch [40/200], Loss: 0.0235\n",
            "Epoch [50/200], Loss: 0.0116\n",
            "Epoch [60/200], Loss: 0.3084\n",
            "Epoch [70/200], Loss: 0.2640\n",
            "Epoch [80/200], Loss: 0.0007\n",
            "Epoch [90/200], Loss: 0.0349\n",
            "Epoch [100/200], Loss: 0.0026\n",
            "Epoch [110/200], Loss: 0.0170\n",
            "Epoch [120/200], Loss: 0.0361\n",
            "Epoch [130/200], Loss: 0.0144\n",
            "Epoch [140/200], Loss: 0.0212\n",
            "Epoch [150/200], Loss: 0.0011\n",
            "Epoch [160/200], Loss: 0.0218\n",
            "Epoch [170/200], Loss: 0.0098\n",
            "Epoch [180/200], Loss: 0.0222\n",
            "Epoch [190/200], Loss: 0.0012\n",
            "Epoch [200/200], Loss: 0.0001\n",
            "Means Squared Error: 0.022401733323931694\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 250, 'learning_rate': 0.001}\n",
            "Epoch [10/250], Loss: 2536.0244\n",
            "Epoch [20/250], Loss: 917.3912\n",
            "Epoch [30/250], Loss: 52.9533\n",
            "Epoch [40/250], Loss: 0.2457\n",
            "Epoch [50/250], Loss: 54.0300\n",
            "Epoch [60/250], Loss: 0.6485\n",
            "Epoch [70/250], Loss: 2.9475\n",
            "Epoch [80/250], Loss: 42.2902\n",
            "Epoch [90/250], Loss: 1.9650\n",
            "Epoch [100/250], Loss: 5.0154\n",
            "Epoch [110/250], Loss: 0.1995\n",
            "Epoch [120/250], Loss: 1.3509\n",
            "Epoch [130/250], Loss: 1.4308\n",
            "Epoch [140/250], Loss: 5.6825\n",
            "Epoch [150/250], Loss: 1.3478\n",
            "Epoch [160/250], Loss: 0.6705\n",
            "Epoch [170/250], Loss: 0.4030\n",
            "Epoch [180/250], Loss: 0.0411\n",
            "Epoch [190/250], Loss: 0.5131\n",
            "Epoch [200/250], Loss: 0.0091\n",
            "Epoch [210/250], Loss: 0.0196\n",
            "Epoch [220/250], Loss: 0.1580\n",
            "Epoch [230/250], Loss: 0.2171\n",
            "Epoch [240/250], Loss: 0.2180\n",
            "Epoch [250/250], Loss: 0.0017\n",
            "Means Squared Error: 0.8160483241081238\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 250, 'learning_rate': 0.005}\n",
            "Epoch [10/250], Loss: 25.6860\n",
            "Epoch [20/250], Loss: 0.6501\n",
            "Epoch [30/250], Loss: 5.3492\n",
            "Epoch [40/250], Loss: 0.3119\n",
            "Epoch [50/250], Loss: 0.3416\n",
            "Epoch [60/250], Loss: 0.7432\n",
            "Epoch [70/250], Loss: 0.0799\n",
            "Epoch [80/250], Loss: 0.0090\n",
            "Epoch [90/250], Loss: 0.2586\n",
            "Epoch [100/250], Loss: 0.0596\n",
            "Epoch [110/250], Loss: 0.3151\n",
            "Epoch [120/250], Loss: 0.1054\n",
            "Epoch [130/250], Loss: 0.0135\n",
            "Epoch [140/250], Loss: 0.0602\n",
            "Epoch [150/250], Loss: 0.2296\n",
            "Epoch [160/250], Loss: 0.3660\n",
            "Epoch [170/250], Loss: 0.0791\n",
            "Epoch [180/250], Loss: 0.0737\n",
            "Epoch [190/250], Loss: 0.0031\n",
            "Epoch [200/250], Loss: 0.0966\n",
            "Epoch [210/250], Loss: 0.1220\n",
            "Epoch [220/250], Loss: 0.4203\n",
            "Epoch [230/250], Loss: 0.6923\n",
            "Epoch [240/250], Loss: 0.4120\n",
            "Epoch [250/250], Loss: 0.0016\n",
            "Means Squared Error: 0.307102769613266\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 250, 'learning_rate': 0.01}\n",
            "Epoch [10/250], Loss: 1.8773\n",
            "Epoch [20/250], Loss: 0.0134\n",
            "Epoch [30/250], Loss: 0.0104\n",
            "Epoch [40/250], Loss: 0.0259\n",
            "Epoch [50/250], Loss: 0.0557\n",
            "Epoch [60/250], Loss: 1.4358\n",
            "Epoch [70/250], Loss: 0.2100\n",
            "Epoch [80/250], Loss: 0.0067\n",
            "Epoch [90/250], Loss: 0.3172\n",
            "Epoch [100/250], Loss: 0.0870\n",
            "Epoch [110/250], Loss: 0.0507\n",
            "Epoch [120/250], Loss: 0.0397\n",
            "Epoch [130/250], Loss: 0.0477\n",
            "Epoch [140/250], Loss: 0.0573\n",
            "Epoch [150/250], Loss: 0.1923\n",
            "Epoch [160/250], Loss: 0.0010\n",
            "Epoch [170/250], Loss: 1.1837\n",
            "Epoch [180/250], Loss: 0.0084\n",
            "Epoch [190/250], Loss: 0.1842\n",
            "Epoch [200/250], Loss: 0.0000\n",
            "Epoch [210/250], Loss: 0.0134\n",
            "Epoch [220/250], Loss: 0.0688\n",
            "Epoch [230/250], Loss: 0.1471\n",
            "Epoch [240/250], Loss: 0.0515\n",
            "Epoch [250/250], Loss: 0.0130\n",
            "Means Squared Error: 0.19102485477924347\n",
            "Testing parameters: {'batch_size': 2, 'epochs': 250, 'learning_rate': 0.05}\n",
            "Epoch [10/250], Loss: 0.0250\n",
            "Epoch [20/250], Loss: 0.4450\n",
            "Epoch [30/250], Loss: 0.1422\n",
            "Epoch [40/250], Loss: 0.0000\n",
            "Epoch [50/250], Loss: 0.6744\n",
            "Epoch [60/250], Loss: 0.1042\n",
            "Epoch [70/250], Loss: 0.0216\n",
            "Epoch [80/250], Loss: 0.0064\n",
            "Epoch [90/250], Loss: 0.4119\n",
            "Epoch [100/250], Loss: 0.0044\n",
            "Epoch [110/250], Loss: 0.1589\n",
            "Epoch [120/250], Loss: 0.0003\n",
            "Epoch [130/250], Loss: 0.0015\n",
            "Epoch [140/250], Loss: 0.0038\n",
            "Epoch [150/250], Loss: 0.0080\n",
            "Epoch [160/250], Loss: 0.0170\n",
            "Epoch [170/250], Loss: 0.0621\n",
            "Epoch [180/250], Loss: 0.0044\n",
            "Epoch [190/250], Loss: 0.0174\n",
            "Epoch [200/250], Loss: 0.0145\n",
            "Epoch [210/250], Loss: 0.0010\n",
            "Epoch [220/250], Loss: 0.0005\n",
            "Epoch [230/250], Loss: 0.0108\n",
            "Epoch [240/250], Loss: 0.0179\n",
            "Epoch [250/250], Loss: 0.0021\n",
            "Means Squared Error: 0.015224060975015163\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 50, 'learning_rate': 0.001}\n",
            "Epoch [10/50], Loss: 6216.4053\n",
            "Epoch [20/50], Loss: 4846.6016\n",
            "Epoch [30/50], Loss: 3505.5522\n",
            "Epoch [40/50], Loss: 1696.4811\n",
            "Epoch [50/50], Loss: 1030.8755\n",
            "Means Squared Error: 1586.688232421875\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 50, 'learning_rate': 0.005}\n",
            "Epoch [10/50], Loss: 897.9512\n",
            "Epoch [20/50], Loss: 88.5742\n",
            "Epoch [30/50], Loss: 8.6487\n",
            "Epoch [40/50], Loss: 6.4739\n",
            "Epoch [50/50], Loss: 2.3404\n",
            "Means Squared Error: 10.364002227783203\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 50, 'learning_rate': 0.01}\n",
            "Epoch [10/50], Loss: 124.8889\n",
            "Epoch [20/50], Loss: 4.0746\n",
            "Epoch [30/50], Loss: 5.7221\n",
            "Epoch [40/50], Loss: 1.0889\n",
            "Epoch [50/50], Loss: 0.3029\n",
            "Means Squared Error: 1.4883869886398315\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 50, 'learning_rate': 0.05}\n",
            "Epoch [10/50], Loss: 0.2835\n",
            "Epoch [20/50], Loss: 0.2485\n",
            "Epoch [30/50], Loss: 0.1039\n",
            "Epoch [40/50], Loss: 0.0439\n",
            "Epoch [50/50], Loss: 0.0485\n",
            "Means Squared Error: 0.2086087018251419\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 100, 'learning_rate': 0.001}\n",
            "Epoch [10/100], Loss: 8106.6670\n",
            "Epoch [20/100], Loss: 3947.8579\n",
            "Epoch [30/100], Loss: 3942.2476\n",
            "Epoch [40/100], Loss: 1671.7242\n",
            "Epoch [50/100], Loss: 847.2490\n",
            "Epoch [60/100], Loss: 717.0082\n",
            "Epoch [70/100], Loss: 253.2136\n",
            "Epoch [80/100], Loss: 161.5518\n",
            "Epoch [90/100], Loss: 105.2667\n",
            "Epoch [100/100], Loss: 97.3310\n",
            "Means Squared Error: 238.11045837402344\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 100, 'learning_rate': 0.005}\n",
            "Epoch [10/100], Loss: 1580.6512\n",
            "Epoch [20/100], Loss: 74.0423\n",
            "Epoch [30/100], Loss: 17.1578\n",
            "Epoch [40/100], Loss: 15.0349\n",
            "Epoch [50/100], Loss: 1.7473\n",
            "Epoch [60/100], Loss: 3.4307\n",
            "Epoch [70/100], Loss: 1.6748\n",
            "Epoch [80/100], Loss: 1.3138\n",
            "Epoch [90/100], Loss: 1.4375\n",
            "Epoch [100/100], Loss: 0.3925\n",
            "Means Squared Error: 1.5123817920684814\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 100, 'learning_rate': 0.01}\n",
            "Epoch [10/100], Loss: 141.4903\n",
            "Epoch [20/100], Loss: 14.1117\n",
            "Epoch [30/100], Loss: 1.8583\n",
            "Epoch [40/100], Loss: 1.9400\n",
            "Epoch [50/100], Loss: 0.6846\n",
            "Epoch [60/100], Loss: 0.2540\n",
            "Epoch [70/100], Loss: 0.0383\n",
            "Epoch [80/100], Loss: 0.3145\n",
            "Epoch [90/100], Loss: 0.0842\n",
            "Epoch [100/100], Loss: 0.0484\n",
            "Means Squared Error: 0.38971972465515137\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 100, 'learning_rate': 0.05}\n",
            "Epoch [10/100], Loss: 1.1003\n",
            "Epoch [20/100], Loss: 0.1295\n",
            "Epoch [30/100], Loss: 0.0475\n",
            "Epoch [40/100], Loss: 0.1134\n",
            "Epoch [50/100], Loss: 0.0524\n",
            "Epoch [60/100], Loss: 0.2044\n",
            "Epoch [70/100], Loss: 0.0652\n",
            "Epoch [80/100], Loss: 0.0882\n",
            "Epoch [90/100], Loss: 0.0374\n",
            "Epoch [100/100], Loss: 0.0652\n",
            "Means Squared Error: 0.2044047713279724\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 150, 'learning_rate': 0.001}\n",
            "Epoch [10/150], Loss: 7478.1821\n",
            "Epoch [20/150], Loss: 5376.8931\n",
            "Epoch [30/150], Loss: 2331.0413\n",
            "Epoch [40/150], Loss: 1425.2092\n",
            "Epoch [50/150], Loss: 862.5233\n",
            "Epoch [60/150], Loss: 570.9098\n",
            "Epoch [70/150], Loss: 382.0994\n",
            "Epoch [80/150], Loss: 118.6060\n",
            "Epoch [90/150], Loss: 104.2043\n",
            "Epoch [100/150], Loss: 148.6315\n",
            "Epoch [110/150], Loss: 40.9501\n",
            "Epoch [120/150], Loss: 24.8288\n",
            "Epoch [130/150], Loss: 57.4100\n",
            "Epoch [140/150], Loss: 21.8035\n",
            "Epoch [150/150], Loss: 13.4845\n",
            "Means Squared Error: 56.123016357421875\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 150, 'learning_rate': 0.005}\n",
            "Epoch [10/150], Loss: 771.5566\n",
            "Epoch [20/150], Loss: 57.8677\n",
            "Epoch [30/150], Loss: 11.4130\n",
            "Epoch [40/150], Loss: 2.5111\n",
            "Epoch [50/150], Loss: 3.8531\n",
            "Epoch [60/150], Loss: 2.4297\n",
            "Epoch [70/150], Loss: 2.7692\n",
            "Epoch [80/150], Loss: 1.2997\n",
            "Epoch [90/150], Loss: 1.2733\n",
            "Epoch [100/150], Loss: 0.4449\n",
            "Epoch [110/150], Loss: 0.6426\n",
            "Epoch [120/150], Loss: 0.5066\n",
            "Epoch [130/150], Loss: 0.4280\n",
            "Epoch [140/150], Loss: 0.3491\n",
            "Epoch [150/150], Loss: 0.1742\n",
            "Means Squared Error: 0.588829517364502\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 150, 'learning_rate': 0.01}\n",
            "Epoch [10/150], Loss: 88.4214\n",
            "Epoch [20/150], Loss: 3.2769\n",
            "Epoch [30/150], Loss: 4.0609\n",
            "Epoch [40/150], Loss: 0.6663\n",
            "Epoch [50/150], Loss: 0.2532\n",
            "Epoch [60/150], Loss: 0.1949\n",
            "Epoch [70/150], Loss: 0.1140\n",
            "Epoch [80/150], Loss: 0.0473\n",
            "Epoch [90/150], Loss: 0.2940\n",
            "Epoch [100/150], Loss: 0.0895\n",
            "Epoch [110/150], Loss: 0.0666\n",
            "Epoch [120/150], Loss: 0.1088\n",
            "Epoch [130/150], Loss: 0.2186\n",
            "Epoch [140/150], Loss: 0.1052\n",
            "Epoch [150/150], Loss: 0.1547\n",
            "Means Squared Error: 0.3598344326019287\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 150, 'learning_rate': 0.05}\n",
            "Epoch [10/150], Loss: 0.8134\n",
            "Epoch [20/150], Loss: 0.1032\n",
            "Epoch [30/150], Loss: 0.3320\n",
            "Epoch [40/150], Loss: 0.1232\n",
            "Epoch [50/150], Loss: 0.1090\n",
            "Epoch [60/150], Loss: 0.0717\n",
            "Epoch [70/150], Loss: 0.0514\n",
            "Epoch [80/150], Loss: 0.1174\n",
            "Epoch [90/150], Loss: 0.1579\n",
            "Epoch [100/150], Loss: 0.0584\n",
            "Epoch [110/150], Loss: 0.0239\n",
            "Epoch [120/150], Loss: 0.2148\n",
            "Epoch [130/150], Loss: 0.0948\n",
            "Epoch [140/150], Loss: 0.0335\n",
            "Epoch [150/150], Loss: 0.1026\n",
            "Means Squared Error: 0.1782616227865219\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 200, 'learning_rate': 0.001}\n",
            "Epoch [10/200], Loss: 7649.1562\n",
            "Epoch [20/200], Loss: 4161.9268\n",
            "Epoch [30/200], Loss: 2697.5881\n",
            "Epoch [40/200], Loss: 1749.1791\n",
            "Epoch [50/200], Loss: 811.9293\n",
            "Epoch [60/200], Loss: 928.5203\n",
            "Epoch [70/200], Loss: 549.8303\n",
            "Epoch [80/200], Loss: 270.7245\n",
            "Epoch [90/200], Loss: 113.0181\n",
            "Epoch [100/200], Loss: 129.8634\n",
            "Epoch [110/200], Loss: 82.4757\n",
            "Epoch [120/200], Loss: 38.4125\n",
            "Epoch [130/200], Loss: 35.6395\n",
            "Epoch [140/200], Loss: 21.3409\n",
            "Epoch [150/200], Loss: 2.8226\n",
            "Epoch [160/200], Loss: 12.4353\n",
            "Epoch [170/200], Loss: 7.6628\n",
            "Epoch [180/200], Loss: 6.7029\n",
            "Epoch [190/200], Loss: 5.5258\n",
            "Epoch [200/200], Loss: 1.0475\n",
            "Means Squared Error: 20.834796905517578\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 200, 'learning_rate': 0.005}\n",
            "Epoch [10/200], Loss: 984.7068\n",
            "Epoch [20/200], Loss: 65.3027\n",
            "Epoch [30/200], Loss: 11.8197\n",
            "Epoch [40/200], Loss: 10.6225\n",
            "Epoch [50/200], Loss: 3.5758\n",
            "Epoch [60/200], Loss: 2.7079\n",
            "Epoch [70/200], Loss: 0.6893\n",
            "Epoch [80/200], Loss: 1.9766\n",
            "Epoch [90/200], Loss: 0.3402\n",
            "Epoch [100/200], Loss: 1.0815\n",
            "Epoch [110/200], Loss: 0.3536\n",
            "Epoch [120/200], Loss: 0.3338\n",
            "Epoch [130/200], Loss: 0.2057\n",
            "Epoch [140/200], Loss: 0.7498\n",
            "Epoch [150/200], Loss: 0.5081\n",
            "Epoch [160/200], Loss: 0.1586\n",
            "Epoch [170/200], Loss: 0.0841\n",
            "Epoch [180/200], Loss: 0.0976\n",
            "Epoch [190/200], Loss: 0.0390\n",
            "Epoch [200/200], Loss: 0.1472\n",
            "Means Squared Error: 0.40625932812690735\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 200, 'learning_rate': 0.01}\n",
            "Epoch [10/200], Loss: 81.1952\n",
            "Epoch [20/200], Loss: 8.6050\n",
            "Epoch [30/200], Loss: 4.3923\n",
            "Epoch [40/200], Loss: 0.4386\n",
            "Epoch [50/200], Loss: 0.2593\n",
            "Epoch [60/200], Loss: 0.1283\n",
            "Epoch [70/200], Loss: 0.5610\n",
            "Epoch [80/200], Loss: 0.0463\n",
            "Epoch [90/200], Loss: 0.1271\n",
            "Epoch [100/200], Loss: 0.2027\n",
            "Epoch [110/200], Loss: 0.0569\n",
            "Epoch [120/200], Loss: 0.5338\n",
            "Epoch [130/200], Loss: 0.1785\n",
            "Epoch [140/200], Loss: 0.0622\n",
            "Epoch [150/200], Loss: 0.0125\n",
            "Epoch [160/200], Loss: 0.3134\n",
            "Epoch [170/200], Loss: 0.1783\n",
            "Epoch [180/200], Loss: 0.0570\n",
            "Epoch [190/200], Loss: 0.1160\n",
            "Epoch [200/200], Loss: 0.1247\n",
            "Means Squared Error: 0.36436861753463745\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 200, 'learning_rate': 0.05}\n",
            "Epoch [10/200], Loss: 0.4793\n",
            "Epoch [20/200], Loss: 0.0474\n",
            "Epoch [30/200], Loss: 0.1011\n",
            "Epoch [40/200], Loss: 0.1072\n",
            "Epoch [50/200], Loss: 0.0638\n",
            "Epoch [60/200], Loss: 0.0454\n",
            "Epoch [70/200], Loss: 0.0829\n",
            "Epoch [80/200], Loss: 0.1187\n",
            "Epoch [90/200], Loss: 0.0381\n",
            "Epoch [100/200], Loss: 0.0531\n",
            "Epoch [110/200], Loss: 0.0604\n",
            "Epoch [120/200], Loss: 0.0797\n",
            "Epoch [130/200], Loss: 0.0374\n",
            "Epoch [140/200], Loss: 0.0280\n",
            "Epoch [150/200], Loss: 0.1051\n",
            "Epoch [160/200], Loss: 0.0300\n",
            "Epoch [170/200], Loss: 0.0235\n",
            "Epoch [180/200], Loss: 0.0224\n",
            "Epoch [190/200], Loss: 0.0637\n",
            "Epoch [200/200], Loss: 0.0250\n",
            "Means Squared Error: 0.12084206193685532\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 250, 'learning_rate': 0.001}\n",
            "Epoch [10/250], Loss: 6597.2671\n",
            "Epoch [20/250], Loss: 4017.3062\n",
            "Epoch [30/250], Loss: 2516.2751\n",
            "Epoch [40/250], Loss: 1527.1139\n",
            "Epoch [50/250], Loss: 708.5240\n",
            "Epoch [60/250], Loss: 743.5845\n",
            "Epoch [70/250], Loss: 393.8576\n",
            "Epoch [80/250], Loss: 268.7621\n",
            "Epoch [90/250], Loss: 107.4247\n",
            "Epoch [100/250], Loss: 102.8445\n",
            "Epoch [110/250], Loss: 71.7079\n",
            "Epoch [120/250], Loss: 49.6510\n",
            "Epoch [130/250], Loss: 34.0351\n",
            "Epoch [140/250], Loss: 11.1130\n",
            "Epoch [150/250], Loss: 29.3447\n",
            "Epoch [160/250], Loss: 19.4062\n",
            "Epoch [170/250], Loss: 6.3244\n",
            "Epoch [180/250], Loss: 6.0993\n",
            "Epoch [190/250], Loss: 5.3042\n",
            "Epoch [200/250], Loss: 8.5896\n",
            "Epoch [210/250], Loss: 5.1436\n",
            "Epoch [220/250], Loss: 7.6686\n",
            "Epoch [230/250], Loss: 3.9010\n",
            "Epoch [240/250], Loss: 2.2519\n",
            "Epoch [250/250], Loss: 6.8099\n",
            "Means Squared Error: 10.916498184204102\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 250, 'learning_rate': 0.005}\n",
            "Epoch [10/250], Loss: 1222.0842\n",
            "Epoch [20/250], Loss: 86.3013\n",
            "Epoch [30/250], Loss: 14.5723\n",
            "Epoch [40/250], Loss: 14.0690\n",
            "Epoch [50/250], Loss: 2.8672\n",
            "Epoch [60/250], Loss: 2.5183\n",
            "Epoch [70/250], Loss: 3.4519\n",
            "Epoch [80/250], Loss: 0.9797\n",
            "Epoch [90/250], Loss: 1.1937\n",
            "Epoch [100/250], Loss: 0.2929\n",
            "Epoch [110/250], Loss: 0.2605\n",
            "Epoch [120/250], Loss: 0.1804\n",
            "Epoch [130/250], Loss: 0.1608\n",
            "Epoch [140/250], Loss: 0.0552\n",
            "Epoch [150/250], Loss: 0.2278\n",
            "Epoch [160/250], Loss: 0.1494\n",
            "Epoch [170/250], Loss: 0.1440\n",
            "Epoch [180/250], Loss: 0.1274\n",
            "Epoch [190/250], Loss: 0.0443\n",
            "Epoch [200/250], Loss: 0.2477\n",
            "Epoch [210/250], Loss: 0.1138\n",
            "Epoch [220/250], Loss: 0.1256\n",
            "Epoch [230/250], Loss: 0.0362\n",
            "Epoch [240/250], Loss: 0.1158\n",
            "Epoch [250/250], Loss: 0.0263\n",
            "Means Squared Error: 0.3576616942882538\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 250, 'learning_rate': 0.01}\n",
            "Epoch [10/250], Loss: 80.4752\n",
            "Epoch [20/250], Loss: 8.1830\n",
            "Epoch [30/250], Loss: 1.8187\n",
            "Epoch [40/250], Loss: 1.5280\n",
            "Epoch [50/250], Loss: 0.5896\n",
            "Epoch [60/250], Loss: 0.2919\n",
            "Epoch [70/250], Loss: 0.2658\n",
            "Epoch [80/250], Loss: 0.0891\n",
            "Epoch [90/250], Loss: 0.1118\n",
            "Epoch [100/250], Loss: 0.1315\n",
            "Epoch [110/250], Loss: 0.0435\n",
            "Epoch [120/250], Loss: 0.0789\n",
            "Epoch [130/250], Loss: 0.0638\n",
            "Epoch [140/250], Loss: 0.0725\n",
            "Epoch [150/250], Loss: 0.0485\n",
            "Epoch [160/250], Loss: 0.0273\n",
            "Epoch [170/250], Loss: 0.1275\n",
            "Epoch [180/250], Loss: 0.0376\n",
            "Epoch [190/250], Loss: 0.0558\n",
            "Epoch [200/250], Loss: 0.3796\n",
            "Epoch [210/250], Loss: 0.0595\n",
            "Epoch [220/250], Loss: 0.1994\n",
            "Epoch [230/250], Loss: 0.1737\n",
            "Epoch [240/250], Loss: 0.0592\n",
            "Epoch [250/250], Loss: 0.0340\n",
            "Means Squared Error: 0.3017217516899109\n",
            "Testing parameters: {'batch_size': 5, 'epochs': 250, 'learning_rate': 0.05}\n",
            "Epoch [10/250], Loss: 0.3384\n",
            "Epoch [20/250], Loss: 0.1537\n",
            "Epoch [30/250], Loss: 0.2992\n",
            "Epoch [40/250], Loss: 0.0250\n",
            "Epoch [50/250], Loss: 0.1156\n",
            "Epoch [60/250], Loss: 0.2401\n",
            "Epoch [70/250], Loss: 0.1339\n",
            "Epoch [80/250], Loss: 0.0289\n",
            "Epoch [90/250], Loss: 0.1978\n",
            "Epoch [100/250], Loss: 0.2489\n",
            "Epoch [110/250], Loss: 0.0397\n",
            "Epoch [120/250], Loss: 0.0936\n",
            "Epoch [130/250], Loss: 0.0426\n",
            "Epoch [140/250], Loss: 0.0105\n",
            "Epoch [150/250], Loss: 0.0588\n",
            "Epoch [160/250], Loss: 0.0485\n",
            "Epoch [170/250], Loss: 0.0160\n",
            "Epoch [180/250], Loss: 0.0328\n",
            "Epoch [190/250], Loss: 0.0527\n",
            "Epoch [200/250], Loss: 0.0187\n",
            "Epoch [210/250], Loss: 0.0595\n",
            "Epoch [220/250], Loss: 0.0033\n",
            "Epoch [230/250], Loss: 0.0149\n",
            "Epoch [240/250], Loss: 0.0153\n",
            "Epoch [250/250], Loss: 0.0437\n",
            "Means Squared Error: 0.08752574771642685\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 50, 'learning_rate': 0.001}\n",
            "Epoch [10/50], Loss: 10309.8398\n",
            "Epoch [20/50], Loss: 7388.1704\n",
            "Epoch [30/50], Loss: 4722.7881\n",
            "Epoch [40/50], Loss: 4157.7490\n",
            "Epoch [50/50], Loss: 2490.6821\n",
            "Means Squared Error: 4104.27734375\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 50, 'learning_rate': 0.005}\n",
            "Epoch [10/50], Loss: 2680.5659\n",
            "Epoch [20/50], Loss: 728.0111\n",
            "Epoch [30/50], Loss: 315.7184\n",
            "Epoch [40/50], Loss: 37.8523\n",
            "Epoch [50/50], Loss: 10.0504\n",
            "Means Squared Error: 82.28363037109375\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 50, 'learning_rate': 0.01}\n",
            "Epoch [10/50], Loss: 824.0089\n",
            "Epoch [20/50], Loss: 45.1885\n",
            "Epoch [30/50], Loss: 10.9565\n",
            "Epoch [40/50], Loss: 2.8686\n",
            "Epoch [50/50], Loss: 2.0994\n",
            "Means Squared Error: 8.61220932006836\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 50, 'learning_rate': 0.05}\n",
            "Epoch [10/50], Loss: 1.5122\n",
            "Epoch [20/50], Loss: 1.0480\n",
            "Epoch [30/50], Loss: 0.2090\n",
            "Epoch [40/50], Loss: 0.0208\n",
            "Epoch [50/50], Loss: 0.2653\n",
            "Means Squared Error: 0.513212263584137\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 100, 'learning_rate': 0.001}\n",
            "Epoch [10/100], Loss: 10537.9375\n",
            "Epoch [20/100], Loss: 7131.7178\n",
            "Epoch [30/100], Loss: 5716.8486\n",
            "Epoch [40/100], Loss: 4403.7642\n",
            "Epoch [50/100], Loss: 3186.6084\n",
            "Epoch [60/100], Loss: 3305.3079\n",
            "Epoch [70/100], Loss: 1678.9814\n",
            "Epoch [80/100], Loss: 1294.0527\n",
            "Epoch [90/100], Loss: 1107.6511\n",
            "Epoch [100/100], Loss: 883.1213\n",
            "Means Squared Error: 1343.673828125\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 100, 'learning_rate': 0.005}\n",
            "Epoch [10/100], Loss: 2574.2681\n",
            "Epoch [20/100], Loss: 642.8464\n",
            "Epoch [30/100], Loss: 144.5012\n",
            "Epoch [40/100], Loss: 38.1423\n",
            "Epoch [50/100], Loss: 23.5383\n",
            "Epoch [60/100], Loss: 21.8550\n",
            "Epoch [70/100], Loss: 3.8776\n",
            "Epoch [80/100], Loss: 5.1087\n",
            "Epoch [90/100], Loss: 2.5237\n",
            "Epoch [100/100], Loss: 7.8749\n",
            "Means Squared Error: 8.628477096557617\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 100, 'learning_rate': 0.01}\n",
            "Epoch [10/100], Loss: 851.7974\n",
            "Epoch [20/100], Loss: 141.9540\n",
            "Epoch [30/100], Loss: 16.5012\n",
            "Epoch [40/100], Loss: 5.1962\n",
            "Epoch [50/100], Loss: 1.0995\n",
            "Epoch [60/100], Loss: 1.4023\n",
            "Epoch [70/100], Loss: 2.3797\n",
            "Epoch [80/100], Loss: 1.3813\n",
            "Epoch [90/100], Loss: 1.0325\n",
            "Epoch [100/100], Loss: 0.3734\n",
            "Means Squared Error: 1.2758030891418457\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 100, 'learning_rate': 0.05}\n",
            "Epoch [10/100], Loss: 3.3859\n",
            "Epoch [20/100], Loss: 0.5086\n",
            "Epoch [30/100], Loss: 0.4971\n",
            "Epoch [40/100], Loss: 0.1298\n",
            "Epoch [50/100], Loss: 0.0923\n",
            "Epoch [60/100], Loss: 0.6076\n",
            "Epoch [70/100], Loss: 0.1331\n",
            "Epoch [80/100], Loss: 0.0737\n",
            "Epoch [90/100], Loss: 0.1084\n",
            "Epoch [100/100], Loss: 0.1402\n",
            "Means Squared Error: 0.31335651874542236\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 150, 'learning_rate': 0.001}\n",
            "Epoch [10/150], Loss: 9293.8574\n",
            "Epoch [20/150], Loss: 7404.3491\n",
            "Epoch [30/150], Loss: 6411.9453\n",
            "Epoch [40/150], Loss: 4425.6587\n",
            "Epoch [50/150], Loss: 2913.2554\n",
            "Epoch [60/150], Loss: 1498.9830\n",
            "Epoch [70/150], Loss: 1933.1897\n",
            "Epoch [80/150], Loss: 1171.5510\n",
            "Epoch [90/150], Loss: 810.3519\n",
            "Epoch [100/150], Loss: 741.0229\n",
            "Epoch [110/150], Loss: 524.2193\n",
            "Epoch [120/150], Loss: 512.6503\n",
            "Epoch [130/150], Loss: 405.4918\n",
            "Epoch [140/150], Loss: 129.4024\n",
            "Epoch [150/150], Loss: 185.3087\n",
            "Means Squared Error: 487.642822265625\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 150, 'learning_rate': 0.005}\n",
            "Epoch [10/150], Loss: 2939.8823\n",
            "Epoch [20/150], Loss: 838.0431\n",
            "Epoch [30/150], Loss: 153.9934\n",
            "Epoch [40/150], Loss: 135.9154\n",
            "Epoch [50/150], Loss: 26.0169\n",
            "Epoch [60/150], Loss: 8.4253\n",
            "Epoch [70/150], Loss: 15.1372\n",
            "Epoch [80/150], Loss: 4.6883\n",
            "Epoch [90/150], Loss: 2.9487\n",
            "Epoch [100/150], Loss: 3.6931\n",
            "Epoch [110/150], Loss: 3.2809\n",
            "Epoch [120/150], Loss: 0.7381\n",
            "Epoch [130/150], Loss: 0.2542\n",
            "Epoch [140/150], Loss: 1.1975\n",
            "Epoch [150/150], Loss: 0.8253\n",
            "Means Squared Error: 2.8709464073181152\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 150, 'learning_rate': 0.01}\n",
            "Epoch [10/150], Loss: 967.8949\n",
            "Epoch [20/150], Loss: 41.8005\n",
            "Epoch [30/150], Loss: 6.8147\n",
            "Epoch [40/150], Loss: 10.3899\n",
            "Epoch [50/150], Loss: 4.4731\n",
            "Epoch [60/150], Loss: 1.4799\n",
            "Epoch [70/150], Loss: 1.8441\n",
            "Epoch [80/150], Loss: 0.9390\n",
            "Epoch [90/150], Loss: 0.4856\n",
            "Epoch [100/150], Loss: 0.2200\n",
            "Epoch [110/150], Loss: 0.7133\n",
            "Epoch [120/150], Loss: 0.0207\n",
            "Epoch [130/150], Loss: 0.0596\n",
            "Epoch [140/150], Loss: 0.1404\n",
            "Epoch [150/150], Loss: 0.0273\n",
            "Means Squared Error: 0.5082425475120544\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 150, 'learning_rate': 0.05}\n",
            "Epoch [10/150], Loss: 2.7357\n",
            "Epoch [20/150], Loss: 1.0637\n",
            "Epoch [30/150], Loss: 0.2675\n",
            "Epoch [40/150], Loss: 0.5248\n",
            "Epoch [50/150], Loss: 0.1304\n",
            "Epoch [60/150], Loss: 0.2073\n",
            "Epoch [70/150], Loss: 0.0432\n",
            "Epoch [80/150], Loss: 0.0888\n",
            "Epoch [90/150], Loss: 0.1930\n",
            "Epoch [100/150], Loss: 0.5232\n",
            "Epoch [110/150], Loss: 0.1197\n",
            "Epoch [120/150], Loss: 0.0299\n",
            "Epoch [130/150], Loss: 0.0811\n",
            "Epoch [140/150], Loss: 0.0446\n",
            "Epoch [150/150], Loss: 0.0442\n",
            "Means Squared Error: 0.2503169775009155\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 200, 'learning_rate': 0.001}\n",
            "Epoch [10/200], Loss: 8915.6641\n",
            "Epoch [20/200], Loss: 7369.2627\n",
            "Epoch [30/200], Loss: 6041.8872\n",
            "Epoch [40/200], Loss: 4264.9883\n",
            "Epoch [50/200], Loss: 2644.5415\n",
            "Epoch [60/200], Loss: 2119.2104\n",
            "Epoch [70/200], Loss: 2358.3801\n",
            "Epoch [80/200], Loss: 1335.9816\n",
            "Epoch [90/200], Loss: 808.3463\n",
            "Epoch [100/200], Loss: 871.9285\n",
            "Epoch [110/200], Loss: 715.2592\n",
            "Epoch [120/200], Loss: 486.0350\n",
            "Epoch [130/200], Loss: 511.5757\n",
            "Epoch [140/200], Loss: 304.7304\n",
            "Epoch [150/200], Loss: 174.1784\n",
            "Epoch [160/200], Loss: 107.7315\n",
            "Epoch [170/200], Loss: 23.3562\n",
            "Epoch [180/200], Loss: 71.9795\n",
            "Epoch [190/200], Loss: 30.7272\n",
            "Epoch [200/200], Loss: 32.1634\n",
            "Means Squared Error: 192.52816772460938\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 200, 'learning_rate': 0.005}\n",
            "Epoch [10/200], Loss: 2737.6492\n",
            "Epoch [20/200], Loss: 729.5494\n",
            "Epoch [30/200], Loss: 410.5350\n",
            "Epoch [40/200], Loss: 45.5821\n",
            "Epoch [50/200], Loss: 11.0458\n",
            "Epoch [60/200], Loss: 12.6011\n",
            "Epoch [70/200], Loss: 8.9888\n",
            "Epoch [80/200], Loss: 4.3566\n",
            "Epoch [90/200], Loss: 3.7423\n",
            "Epoch [100/200], Loss: 3.2325\n",
            "Epoch [110/200], Loss: 1.0659\n",
            "Epoch [120/200], Loss: 2.1585\n",
            "Epoch [130/200], Loss: 4.3719\n",
            "Epoch [140/200], Loss: 0.5786\n",
            "Epoch [150/200], Loss: 1.0176\n",
            "Epoch [160/200], Loss: 1.4828\n",
            "Epoch [170/200], Loss: 0.9335\n",
            "Epoch [180/200], Loss: 0.8620\n",
            "Epoch [190/200], Loss: 0.5158\n",
            "Epoch [200/200], Loss: 0.4445\n",
            "Means Squared Error: 1.3115016222000122\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 200, 'learning_rate': 0.01}\n",
            "Epoch [10/200], Loss: 905.9373\n",
            "Epoch [20/200], Loss: 68.4518\n",
            "Epoch [30/200], Loss: 8.8213\n",
            "Epoch [40/200], Loss: 8.0120\n",
            "Epoch [50/200], Loss: 2.2325\n",
            "Epoch [60/200], Loss: 1.4708\n",
            "Epoch [70/200], Loss: 1.3515\n",
            "Epoch [80/200], Loss: 0.4846\n",
            "Epoch [90/200], Loss: 1.2563\n",
            "Epoch [100/200], Loss: 0.1975\n",
            "Epoch [110/200], Loss: 0.5594\n",
            "Epoch [120/200], Loss: 0.0776\n",
            "Epoch [130/200], Loss: 0.1878\n",
            "Epoch [140/200], Loss: 0.2144\n",
            "Epoch [150/200], Loss: 0.2996\n",
            "Epoch [160/200], Loss: 0.1614\n",
            "Epoch [170/200], Loss: 0.1273\n",
            "Epoch [180/200], Loss: 0.0933\n",
            "Epoch [190/200], Loss: 0.0659\n",
            "Epoch [200/200], Loss: 0.6195\n",
            "Means Squared Error: 0.4062208831310272\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 200, 'learning_rate': 0.05}\n",
            "Epoch [10/200], Loss: 4.3833\n",
            "Epoch [20/200], Loss: 0.3868\n",
            "Epoch [30/200], Loss: 0.2072\n",
            "Epoch [40/200], Loss: 0.0655\n",
            "Epoch [50/200], Loss: 0.1551\n",
            "Epoch [60/200], Loss: 0.0997\n",
            "Epoch [70/200], Loss: 0.1726\n",
            "Epoch [80/200], Loss: 0.1072\n",
            "Epoch [90/200], Loss: 0.3674\n",
            "Epoch [100/200], Loss: 0.0728\n",
            "Epoch [110/200], Loss: 0.3117\n",
            "Epoch [120/200], Loss: 0.0395\n",
            "Epoch [130/200], Loss: 0.0402\n",
            "Epoch [140/200], Loss: 0.0581\n",
            "Epoch [150/200], Loss: 0.0686\n",
            "Epoch [160/200], Loss: 0.0793\n",
            "Epoch [170/200], Loss: 0.0444\n",
            "Epoch [180/200], Loss: 0.0622\n",
            "Epoch [190/200], Loss: 0.1043\n",
            "Epoch [200/200], Loss: 0.0119\n",
            "Means Squared Error: 0.23081150650978088\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 250, 'learning_rate': 0.001}\n",
            "Epoch [10/250], Loss: 9461.2744\n",
            "Epoch [20/250], Loss: 6716.1226\n",
            "Epoch [30/250], Loss: 6370.9780\n",
            "Epoch [40/250], Loss: 4911.1455\n",
            "Epoch [50/250], Loss: 2939.7737\n",
            "Epoch [60/250], Loss: 2812.2915\n",
            "Epoch [70/250], Loss: 1724.8138\n",
            "Epoch [80/250], Loss: 773.0832\n",
            "Epoch [90/250], Loss: 763.8717\n",
            "Epoch [100/250], Loss: 785.7477\n",
            "Epoch [110/250], Loss: 514.4082\n",
            "Epoch [120/250], Loss: 568.2394\n",
            "Epoch [130/250], Loss: 320.2077\n",
            "Epoch [140/250], Loss: 171.8816\n",
            "Epoch [150/250], Loss: 165.8855\n",
            "Epoch [160/250], Loss: 171.2296\n",
            "Epoch [170/250], Loss: 194.5590\n",
            "Epoch [180/250], Loss: 117.1118\n",
            "Epoch [190/250], Loss: 88.9746\n",
            "Epoch [200/250], Loss: 100.6587\n",
            "Epoch [210/250], Loss: 49.1584\n",
            "Epoch [220/250], Loss: 122.9191\n",
            "Epoch [230/250], Loss: 45.4626\n",
            "Epoch [240/250], Loss: 18.3040\n",
            "Epoch [250/250], Loss: 18.7279\n",
            "Means Squared Error: 85.06969451904297\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 250, 'learning_rate': 0.005}\n",
            "Epoch [10/250], Loss: 3879.2188\n",
            "Epoch [20/250], Loss: 757.2156\n",
            "Epoch [30/250], Loss: 48.8740\n",
            "Epoch [40/250], Loss: 66.3212\n",
            "Epoch [50/250], Loss: 15.5094\n",
            "Epoch [60/250], Loss: 4.6488\n",
            "Epoch [70/250], Loss: 17.2835\n",
            "Epoch [80/250], Loss: 7.5921\n",
            "Epoch [90/250], Loss: 2.8973\n",
            "Epoch [100/250], Loss: 6.1195\n",
            "Epoch [110/250], Loss: 1.3495\n",
            "Epoch [120/250], Loss: 3.1398\n",
            "Epoch [130/250], Loss: 1.7463\n",
            "Epoch [140/250], Loss: 1.4023\n",
            "Epoch [150/250], Loss: 1.6672\n",
            "Epoch [160/250], Loss: 1.2020\n",
            "Epoch [170/250], Loss: 0.6844\n",
            "Epoch [180/250], Loss: 0.0155\n",
            "Epoch [190/250], Loss: 0.2739\n",
            "Epoch [200/250], Loss: 0.0810\n",
            "Epoch [210/250], Loss: 0.7481\n",
            "Epoch [220/250], Loss: 0.2952\n",
            "Epoch [230/250], Loss: 0.1370\n",
            "Epoch [240/250], Loss: 0.7781\n",
            "Epoch [250/250], Loss: 0.3027\n",
            "Means Squared Error: 0.7458332180976868\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 250, 'learning_rate': 0.01}\n",
            "Epoch [10/250], Loss: 795.5976\n",
            "Epoch [20/250], Loss: 86.9381\n",
            "Epoch [30/250], Loss: 4.4334\n",
            "Epoch [40/250], Loss: 3.3436\n",
            "Epoch [50/250], Loss: 0.6434\n",
            "Epoch [60/250], Loss: 2.9384\n",
            "Epoch [70/250], Loss: 1.1749\n",
            "Epoch [80/250], Loss: 1.2037\n",
            "Epoch [90/250], Loss: 0.9092\n",
            "Epoch [100/250], Loss: 0.3607\n",
            "Epoch [110/250], Loss: 0.6030\n",
            "Epoch [120/250], Loss: 0.4758\n",
            "Epoch [130/250], Loss: 0.0426\n",
            "Epoch [140/250], Loss: 0.3235\n",
            "Epoch [150/250], Loss: 0.2275\n",
            "Epoch [160/250], Loss: 0.1633\n",
            "Epoch [170/250], Loss: 0.1334\n",
            "Epoch [180/250], Loss: 0.0691\n",
            "Epoch [190/250], Loss: 0.2394\n",
            "Epoch [200/250], Loss: 0.1560\n",
            "Epoch [210/250], Loss: 0.0731\n",
            "Epoch [220/250], Loss: 0.0185\n",
            "Epoch [230/250], Loss: 0.0327\n",
            "Epoch [240/250], Loss: 0.1708\n",
            "Epoch [250/250], Loss: 0.0501\n",
            "Means Squared Error: 0.36070531606674194\n",
            "Testing parameters: {'batch_size': 10, 'epochs': 250, 'learning_rate': 0.05}\n",
            "Epoch [10/250], Loss: 8.5938\n",
            "Epoch [20/250], Loss: 0.0386\n",
            "Epoch [30/250], Loss: 0.2431\n",
            "Epoch [40/250], Loss: 0.1204\n",
            "Epoch [50/250], Loss: 0.2064\n",
            "Epoch [60/250], Loss: 0.1186\n",
            "Epoch [70/250], Loss: 0.0209\n",
            "Epoch [80/250], Loss: 0.1566\n",
            "Epoch [90/250], Loss: 0.3479\n",
            "Epoch [100/250], Loss: 0.0955\n",
            "Epoch [110/250], Loss: 0.1121\n",
            "Epoch [120/250], Loss: 0.0283\n",
            "Epoch [130/250], Loss: 0.1919\n",
            "Epoch [140/250], Loss: 0.0407\n",
            "Epoch [150/250], Loss: 0.1294\n",
            "Epoch [160/250], Loss: 0.0560\n",
            "Epoch [170/250], Loss: 0.2545\n",
            "Epoch [180/250], Loss: 0.0873\n",
            "Epoch [190/250], Loss: 0.0673\n",
            "Epoch [200/250], Loss: 0.0571\n",
            "Epoch [210/250], Loss: 0.3218\n",
            "Epoch [220/250], Loss: 0.1049\n",
            "Epoch [230/250], Loss: 0.0506\n",
            "Epoch [240/250], Loss: 0.0541\n",
            "Epoch [250/250], Loss: 0.0061\n",
            "Means Squared Error: 0.15020649135112762\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 50, 'learning_rate': 0.001}\n",
            "Epoch [10/50], Loss: 13003.8223\n",
            "Epoch [20/50], Loss: 8913.3037\n",
            "Epoch [30/50], Loss: 8870.6221\n",
            "Epoch [40/50], Loss: 7948.1562\n",
            "Epoch [50/50], Loss: 8019.0850\n",
            "Means Squared Error: 7875.75390625\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 50, 'learning_rate': 0.005}\n",
            "Epoch [10/50], Loss: 5874.8701\n",
            "Epoch [20/50], Loss: 3324.8276\n",
            "Epoch [30/50], Loss: 2213.6365\n",
            "Epoch [40/50], Loss: 1176.7278\n",
            "Epoch [50/50], Loss: 650.9122\n",
            "Means Squared Error: 1068.1444091796875\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 50, 'learning_rate': 0.01}\n",
            "Epoch [10/50], Loss: 3698.2336\n",
            "Epoch [20/50], Loss: 2109.0439\n",
            "Epoch [30/50], Loss: 350.4201\n",
            "Epoch [40/50], Loss: 74.2896\n",
            "Epoch [50/50], Loss: 73.2839\n",
            "Means Squared Error: 145.7520294189453\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 50, 'learning_rate': 0.05}\n",
            "Epoch [10/50], Loss: 63.5165\n",
            "Epoch [20/50], Loss: 2.2461\n",
            "Epoch [30/50], Loss: 1.2006\n",
            "Epoch [40/50], Loss: 0.8526\n",
            "Epoch [50/50], Loss: 0.1055\n",
            "Means Squared Error: 0.9779057502746582\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 100, 'learning_rate': 0.001}\n",
            "Epoch [10/100], Loss: 11629.2607\n",
            "Epoch [20/100], Loss: 10048.7109\n",
            "Epoch [30/100], Loss: 8933.9609\n",
            "Epoch [40/100], Loss: 8483.8242\n",
            "Epoch [50/100], Loss: 7583.3838\n",
            "Epoch [60/100], Loss: 7474.2446\n",
            "Epoch [70/100], Loss: 6060.3345\n",
            "Epoch [80/100], Loss: 4545.2466\n",
            "Epoch [90/100], Loss: 3967.2012\n",
            "Epoch [100/100], Loss: 3850.7046\n",
            "Means Squared Error: 4827.29296875\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 100, 'learning_rate': 0.005}\n",
            "Epoch [10/100], Loss: 6380.4326\n",
            "Epoch [20/100], Loss: 3929.0679\n",
            "Epoch [30/100], Loss: 1346.2969\n",
            "Epoch [40/100], Loss: 907.5119\n",
            "Epoch [50/100], Loss: 399.4280\n",
            "Epoch [60/100], Loss: 253.5187\n",
            "Epoch [70/100], Loss: 245.0184\n",
            "Epoch [80/100], Loss: 226.7551\n",
            "Epoch [90/100], Loss: 131.9910\n",
            "Epoch [100/100], Loss: 35.1256\n",
            "Means Squared Error: 158.90589904785156\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 100, 'learning_rate': 0.01}\n",
            "Epoch [10/100], Loss: 4060.9114\n",
            "Epoch [20/100], Loss: 1082.2211\n",
            "Epoch [30/100], Loss: 199.8126\n",
            "Epoch [40/100], Loss: 116.9802\n",
            "Epoch [50/100], Loss: 45.4271\n",
            "Epoch [60/100], Loss: 19.3763\n",
            "Epoch [70/100], Loss: 11.7650\n",
            "Epoch [80/100], Loss: 4.5275\n",
            "Epoch [90/100], Loss: 5.8441\n",
            "Epoch [100/100], Loss: 3.1354\n",
            "Means Squared Error: 10.586883544921875\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 100, 'learning_rate': 0.05}\n",
            "Epoch [10/100], Loss: 48.5615\n",
            "Epoch [20/100], Loss: 19.8221\n",
            "Epoch [30/100], Loss: 4.5182\n",
            "Epoch [40/100], Loss: 1.2886\n",
            "Epoch [50/100], Loss: 0.2116\n",
            "Epoch [60/100], Loss: 0.4758\n",
            "Epoch [70/100], Loss: 0.3028\n",
            "Epoch [80/100], Loss: 0.3016\n",
            "Epoch [90/100], Loss: 0.1108\n",
            "Epoch [100/100], Loss: 0.0706\n",
            "Means Squared Error: 0.46321576833724976\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 150, 'learning_rate': 0.001}\n",
            "Epoch [10/150], Loss: 11210.1230\n",
            "Epoch [20/150], Loss: 10592.3818\n",
            "Epoch [30/150], Loss: 8348.7393\n",
            "Epoch [40/150], Loss: 7292.8188\n",
            "Epoch [50/150], Loss: 5756.0225\n",
            "Epoch [60/150], Loss: 5409.6782\n",
            "Epoch [70/150], Loss: 5718.6353\n",
            "Epoch [80/150], Loss: 4767.5303\n",
            "Epoch [90/150], Loss: 4107.3726\n",
            "Epoch [100/150], Loss: 3811.5332\n",
            "Epoch [110/150], Loss: 3438.0630\n",
            "Epoch [120/150], Loss: 2513.5254\n",
            "Epoch [130/150], Loss: 2349.2979\n",
            "Epoch [140/150], Loss: 1843.0896\n",
            "Epoch [150/150], Loss: 1995.8943\n",
            "Means Squared Error: 2994.824462890625\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 150, 'learning_rate': 0.005}\n",
            "Epoch [10/150], Loss: 6598.9658\n",
            "Epoch [20/150], Loss: 3911.3293\n",
            "Epoch [30/150], Loss: 2570.2905\n",
            "Epoch [40/150], Loss: 1014.6434\n",
            "Epoch [50/150], Loss: 689.1426\n",
            "Epoch [60/150], Loss: 631.9240\n",
            "Epoch [70/150], Loss: 246.5139\n",
            "Epoch [80/150], Loss: 76.8742\n",
            "Epoch [90/150], Loss: 40.7746\n",
            "Epoch [100/150], Loss: 38.3137\n",
            "Epoch [110/150], Loss: 49.4024\n",
            "Epoch [120/150], Loss: 25.2352\n",
            "Epoch [130/150], Loss: 39.7702\n",
            "Epoch [140/150], Loss: 12.9541\n",
            "Epoch [150/150], Loss: 11.3241\n",
            "Means Squared Error: 34.2873649597168\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 150, 'learning_rate': 0.01}\n",
            "Epoch [10/150], Loss: 4293.3921\n",
            "Epoch [20/150], Loss: 858.2687\n",
            "Epoch [30/150], Loss: 360.8905\n",
            "Epoch [40/150], Loss: 69.2269\n",
            "Epoch [50/150], Loss: 68.1493\n",
            "Epoch [60/150], Loss: 41.8409\n",
            "Epoch [70/150], Loss: 17.5565\n",
            "Epoch [80/150], Loss: 12.1539\n",
            "Epoch [90/150], Loss: 10.3686\n",
            "Epoch [100/150], Loss: 11.9814\n",
            "Epoch [110/150], Loss: 6.2019\n",
            "Epoch [120/150], Loss: 5.4931\n",
            "Epoch [130/150], Loss: 1.8202\n",
            "Epoch [140/150], Loss: 0.5549\n",
            "Epoch [150/150], Loss: 1.4987\n",
            "Means Squared Error: 6.327274322509766\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 150, 'learning_rate': 0.05}\n",
            "Epoch [10/150], Loss: 126.0583\n",
            "Epoch [20/150], Loss: 9.5406\n",
            "Epoch [30/150], Loss: 2.0095\n",
            "Epoch [40/150], Loss: 0.3176\n",
            "Epoch [50/150], Loss: 0.3817\n",
            "Epoch [60/150], Loss: 0.1106\n",
            "Epoch [70/150], Loss: 0.3415\n",
            "Epoch [80/150], Loss: 0.0896\n",
            "Epoch [90/150], Loss: 0.1654\n",
            "Epoch [100/150], Loss: 0.2375\n",
            "Epoch [110/150], Loss: 0.1683\n",
            "Epoch [120/150], Loss: 0.1221\n",
            "Epoch [130/150], Loss: 0.0408\n",
            "Epoch [140/150], Loss: 0.0377\n",
            "Epoch [150/150], Loss: 0.3027\n",
            "Means Squared Error: 0.2908793091773987\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 200, 'learning_rate': 0.001}\n",
            "Epoch [10/200], Loss: 12203.1357\n",
            "Epoch [20/200], Loss: 9602.8477\n",
            "Epoch [30/200], Loss: 8786.1016\n",
            "Epoch [40/200], Loss: 8504.1924\n",
            "Epoch [50/200], Loss: 6913.3008\n",
            "Epoch [60/200], Loss: 6706.3179\n",
            "Epoch [70/200], Loss: 5951.3877\n",
            "Epoch [80/200], Loss: 4644.2944\n",
            "Epoch [90/200], Loss: 4281.8145\n",
            "Epoch [100/200], Loss: 3795.2212\n",
            "Epoch [110/200], Loss: 2331.8071\n",
            "Epoch [120/200], Loss: 3057.7678\n",
            "Epoch [130/200], Loss: 3009.5815\n",
            "Epoch [140/200], Loss: 2117.4822\n",
            "Epoch [150/200], Loss: 2463.6074\n",
            "Epoch [160/200], Loss: 1736.6039\n",
            "Epoch [170/200], Loss: 1631.5933\n",
            "Epoch [180/200], Loss: 1572.2468\n",
            "Epoch [190/200], Loss: 1322.2689\n",
            "Epoch [200/200], Loss: 1375.6107\n",
            "Means Squared Error: 1864.4298095703125\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 200, 'learning_rate': 0.005}\n",
            "Epoch [10/200], Loss: 7077.8701\n",
            "Epoch [20/200], Loss: 4572.1743\n",
            "Epoch [30/200], Loss: 2112.8550\n",
            "Epoch [40/200], Loss: 1517.2498\n",
            "Epoch [50/200], Loss: 535.8576\n",
            "Epoch [60/200], Loss: 344.3953\n",
            "Epoch [70/200], Loss: 152.3539\n",
            "Epoch [80/200], Loss: 97.7160\n",
            "Epoch [90/200], Loss: 129.2049\n",
            "Epoch [100/200], Loss: 76.1420\n",
            "Epoch [110/200], Loss: 18.6919\n",
            "Epoch [120/200], Loss: 11.1685\n",
            "Epoch [130/200], Loss: 8.4749\n",
            "Epoch [140/200], Loss: 11.9860\n",
            "Epoch [150/200], Loss: 15.1022\n",
            "Epoch [160/200], Loss: 9.5776\n",
            "Epoch [170/200], Loss: 2.5424\n",
            "Epoch [180/200], Loss: 5.5387\n",
            "Epoch [190/200], Loss: 5.8172\n",
            "Epoch [200/200], Loss: 2.4539\n",
            "Means Squared Error: 14.167261123657227\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 200, 'learning_rate': 0.01}\n",
            "Epoch [10/200], Loss: 3949.9292\n",
            "Epoch [20/200], Loss: 1488.2991\n",
            "Epoch [30/200], Loss: 239.7567\n",
            "Epoch [40/200], Loss: 124.7650\n",
            "Epoch [50/200], Loss: 70.0739\n",
            "Epoch [60/200], Loss: 6.7771\n",
            "Epoch [70/200], Loss: 10.3483\n",
            "Epoch [80/200], Loss: 3.7004\n",
            "Epoch [90/200], Loss: 7.7304\n",
            "Epoch [100/200], Loss: 3.2152\n",
            "Epoch [110/200], Loss: 3.0092\n",
            "Epoch [120/200], Loss: 5.7353\n",
            "Epoch [130/200], Loss: 3.0534\n",
            "Epoch [140/200], Loss: 2.2506\n",
            "Epoch [150/200], Loss: 2.5806\n",
            "Epoch [160/200], Loss: 1.7026\n",
            "Epoch [170/200], Loss: 1.1814\n",
            "Epoch [180/200], Loss: 0.2788\n",
            "Epoch [190/200], Loss: 0.5997\n",
            "Epoch [200/200], Loss: 0.6552\n",
            "Means Squared Error: 1.759490728378296\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 200, 'learning_rate': 0.05}\n",
            "Epoch [10/200], Loss: 54.6685\n",
            "Epoch [20/200], Loss: 6.5259\n",
            "Epoch [30/200], Loss: 1.8234\n",
            "Epoch [40/200], Loss: 1.2306\n",
            "Epoch [50/200], Loss: 0.5452\n",
            "Epoch [60/200], Loss: 0.1305\n",
            "Epoch [70/200], Loss: 0.0519\n",
            "Epoch [80/200], Loss: 0.0877\n",
            "Epoch [90/200], Loss: 0.1622\n",
            "Epoch [100/200], Loss: 0.0357\n",
            "Epoch [110/200], Loss: 0.0823\n",
            "Epoch [120/200], Loss: 0.0876\n",
            "Epoch [130/200], Loss: 0.2537\n",
            "Epoch [140/200], Loss: 0.0590\n",
            "Epoch [150/200], Loss: 0.0865\n",
            "Epoch [160/200], Loss: 0.0351\n",
            "Epoch [170/200], Loss: 0.0226\n",
            "Epoch [180/200], Loss: 0.1022\n",
            "Epoch [190/200], Loss: 0.0749\n",
            "Epoch [200/200], Loss: 0.0686\n",
            "Means Squared Error: 0.38680753111839294\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 250, 'learning_rate': 0.001}\n",
            "Epoch [10/250], Loss: 10051.1621\n",
            "Epoch [20/250], Loss: 10575.2920\n",
            "Epoch [30/250], Loss: 7979.5615\n",
            "Epoch [40/250], Loss: 9392.7676\n",
            "Epoch [50/250], Loss: 6427.1006\n",
            "Epoch [60/250], Loss: 6574.2373\n",
            "Epoch [70/250], Loss: 5535.1553\n",
            "Epoch [80/250], Loss: 3945.2024\n",
            "Epoch [90/250], Loss: 3107.2798\n",
            "Epoch [100/250], Loss: 5127.5635\n",
            "Epoch [110/250], Loss: 4221.9858\n",
            "Epoch [120/250], Loss: 2615.6841\n",
            "Epoch [130/250], Loss: 2866.1489\n",
            "Epoch [140/250], Loss: 2454.1226\n",
            "Epoch [150/250], Loss: 1913.2937\n",
            "Epoch [160/250], Loss: 1529.2861\n",
            "Epoch [170/250], Loss: 1519.8801\n",
            "Epoch [180/250], Loss: 1899.9365\n",
            "Epoch [190/250], Loss: 1024.1560\n",
            "Epoch [200/250], Loss: 921.4596\n",
            "Epoch [210/250], Loss: 813.2568\n",
            "Epoch [220/250], Loss: 918.7341\n",
            "Epoch [230/250], Loss: 742.1976\n",
            "Epoch [240/250], Loss: 931.4059\n",
            "Epoch [250/250], Loss: 826.0957\n",
            "Means Squared Error: 1185.963623046875\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 250, 'learning_rate': 0.005}\n",
            "Epoch [10/250], Loss: 6843.6533\n",
            "Epoch [20/250], Loss: 2771.9014\n",
            "Epoch [30/250], Loss: 1929.4056\n",
            "Epoch [40/250], Loss: 1118.5667\n",
            "Epoch [50/250], Loss: 803.2686\n",
            "Epoch [60/250], Loss: 282.2281\n",
            "Epoch [70/250], Loss: 283.6826\n",
            "Epoch [80/250], Loss: 68.7574\n",
            "Epoch [90/250], Loss: 98.7737\n",
            "Epoch [100/250], Loss: 24.4307\n",
            "Epoch [110/250], Loss: 14.0281\n",
            "Epoch [120/250], Loss: 44.1278\n",
            "Epoch [130/250], Loss: 29.3992\n",
            "Epoch [140/250], Loss: 9.8943\n",
            "Epoch [150/250], Loss: 7.1579\n",
            "Epoch [160/250], Loss: 4.8115\n",
            "Epoch [170/250], Loss: 13.2367\n",
            "Epoch [180/250], Loss: 5.6269\n",
            "Epoch [190/250], Loss: 3.5504\n",
            "Epoch [200/250], Loss: 2.8042\n",
            "Epoch [210/250], Loss: 5.9447\n",
            "Epoch [220/250], Loss: 8.8660\n",
            "Epoch [230/250], Loss: 2.6531\n",
            "Epoch [240/250], Loss: 2.9068\n",
            "Epoch [250/250], Loss: 0.7458\n",
            "Means Squared Error: 8.789475440979004\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 250, 'learning_rate': 0.01}\n",
            "Epoch [10/250], Loss: 4134.9038\n",
            "Epoch [20/250], Loss: 940.3822\n",
            "Epoch [30/250], Loss: 623.8119\n",
            "Epoch [40/250], Loss: 68.0025\n",
            "Epoch [50/250], Loss: 70.3863\n",
            "Epoch [60/250], Loss: 34.7371\n",
            "Epoch [70/250], Loss: 25.8948\n",
            "Epoch [80/250], Loss: 12.9134\n",
            "Epoch [90/250], Loss: 7.9779\n",
            "Epoch [100/250], Loss: 4.6084\n",
            "Epoch [110/250], Loss: 8.6761\n",
            "Epoch [120/250], Loss: 6.8545\n",
            "Epoch [130/250], Loss: 2.5788\n",
            "Epoch [140/250], Loss: 4.9705\n",
            "Epoch [150/250], Loss: 1.0721\n",
            "Epoch [160/250], Loss: 0.9518\n",
            "Epoch [170/250], Loss: 0.3890\n",
            "Epoch [180/250], Loss: 0.7916\n",
            "Epoch [190/250], Loss: 1.1939\n",
            "Epoch [200/250], Loss: 0.8361\n",
            "Epoch [210/250], Loss: 0.4099\n",
            "Epoch [220/250], Loss: 1.7141\n",
            "Epoch [230/250], Loss: 0.5668\n",
            "Epoch [240/250], Loss: 0.3715\n",
            "Epoch [250/250], Loss: 0.2963\n",
            "Means Squared Error: 1.2460321187973022\n",
            "Testing parameters: {'batch_size': 30, 'epochs': 250, 'learning_rate': 0.05}\n",
            "Epoch [10/250], Loss: 39.8568\n",
            "Epoch [20/250], Loss: 1.9489\n",
            "Epoch [30/250], Loss: 0.9300\n",
            "Epoch [40/250], Loss: 0.3463\n",
            "Epoch [50/250], Loss: 0.2094\n",
            "Epoch [60/250], Loss: 0.8516\n",
            "Epoch [70/250], Loss: 0.1724\n",
            "Epoch [80/250], Loss: 0.2048\n",
            "Epoch [90/250], Loss: 0.0857\n",
            "Epoch [100/250], Loss: 0.1877\n",
            "Epoch [110/250], Loss: 0.0414\n",
            "Epoch [120/250], Loss: 0.1619\n",
            "Epoch [130/250], Loss: 0.0385\n",
            "Epoch [140/250], Loss: 0.4861\n",
            "Epoch [150/250], Loss: 0.1811\n",
            "Epoch [160/250], Loss: 0.4442\n",
            "Epoch [170/250], Loss: 0.0959\n",
            "Epoch [180/250], Loss: 0.3780\n",
            "Epoch [190/250], Loss: 0.0981\n",
            "Epoch [200/250], Loss: 0.0196\n",
            "Epoch [210/250], Loss: 0.0595\n",
            "Epoch [220/250], Loss: 0.0920\n",
            "Epoch [230/250], Loss: 0.0544\n",
            "Epoch [240/250], Loss: 0.0146\n",
            "Epoch [250/250], Loss: 0.0156\n",
            "Means Squared Error: 0.3143342435359955\n",
            "Best parameters: {'batch_size': 2, 'epochs': 250, 'learning_rate': 0.05}\n",
            "Best Mean Squared Error: 0.015224060975015163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bs = 2\n",
        "# epochs = 250\n",
        "# lr = 0.05\n",
        "# mse = 0.015224060975015163"
      ],
      "metadata": {
        "id": "-pbSyTuGHjFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}